{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "445c286e-199c-4bda-a99c-b5452e873523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557bffc9-de2e-4c87-9f2f-a1195cd0615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_dict len torch.Size([59, 320])\n",
      "embeddings_dict tensor([[-0.6685, -0.0708, -0.5033,  ...,  0.8954,  0.4106, -0.5340],\n",
      "        [-0.6899, -0.1052,  0.0013,  ...,  0.5112, -0.0517, -0.3438],\n",
      "        [-0.5972, -0.1812,  0.3029,  ...,  0.4339,  0.3076, -0.0928],\n",
      "        ...,\n",
      "        [-0.2558, -0.0050,  0.2232,  ...,  0.4575, -0.0603, -0.1484],\n",
      "        [-0.3865, -0.1068,  0.3723,  ...,  0.4398, -0.3131, -0.0656],\n",
      "        [-0.1832,  0.3298,  0.3563,  ..., -0.0673, -0.4823, -0.1598]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings_vectors_curated_data(folder_path):\n",
    "    # Initialize a list to store the vectors\n",
    "    embeddings = {}\n",
    "\n",
    "    # Traverse through each folder in the specified directory\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a .pt file\n",
    "            if file.endswith('.pt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                embedding = torch.load(file_path)\n",
    "                embeddings[embedding[\"label\"]] = embedding[\"representations\"][6]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings_dict = get_embeddings_vectors_curated_data('curated_dataset/example_embeddings_esm2_reduced_input')\n",
    "print(\"embeddings_dict len\", (embeddings_dict[\"A0A1X7AIY7.1/282-340\"]).shape)\n",
    "print(\"embeddings_dict\", embeddings_dict[\"A0A1X7AIY7.1/282-340\"])\n",
    "len(\"STPIRIFANGRRRVEVLRDNRLIYATSVNAGSQEIDTSSFPQGSYQLTIRIFNGSTLEQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7663a67f-94c0-41de-9de1-81014ea69328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('QIGGEDKADIAPILKEGFVGPGMQINNLLQERGEIVATVICGDNYFNENLDEATDTILGMIGQFNPDIVIAGPSFNAGRYGMACGAVCKAVSEKFNIPTLTGMYIESPGVDGYRKYTYIVETANSAVGMRTALPAMVKLALKLVDGVELGDPKEEGYIARGVRRNYFHAVRGSKRAVDMLIAKINDQPFTTEYPMPTFDRVAPNPHIVDMSKATIALVTSGGIVPKGNPDHIESSSASKFGKYDIEGFTNLTEKTHETAHGGYDPVYANLDADRVLPVDVLRELEAEGVIGKLHRYFYTTVGNGTSVANAKKFAAAIGKELVEAKVDAVILTST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0f1b79-a98e-445a-970b-de457463f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conservation_scores(csv_file):\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, delimiter=',', names=[\n",
    "                     'sequence id', 'conservation score'], header=0)\n",
    "\n",
    "    sequences = df['sequence id'].values\n",
    "    conservation_scores = df['conservation score'].apply(lambda x: np.array(\n",
    "    [float(i) if i != 'nan' else 0.0 for i in x.split()], dtype=np.float32)).values\n",
    "    #print(\"conservation_scores\", conservation_scores)\n",
    "\n",
    "    return sequences, conservation_scores\n",
    "\n",
    "sequences, conservation_scores_tensors = get_conservation_scores('curated_dataset/reduced_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cadad60d-9d2f-4eed-b4cd-20f4b8d57b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conservation_scores_tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ef0555-34a2-4492-892d-86df94a89cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "loss tensor(0.1947, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0858, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0246, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1117, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1404, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0928, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0386, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0536, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0212, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1400, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1286, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0939, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1143, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1100, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1401, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1324, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1223, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0887, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0454, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "label shape torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding shape torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output shape torch.Size([202])\n",
      "output tensor([0.3012, 0.2185, 0.2742, 0.2467, 0.4065, 0.2840, 0.3178, 0.3498, 0.4090,\n",
      "        0.5909, 0.4549, 0.3393, 0.5039, 0.3223, 0.3790, 0.3412, 0.3926, 0.4773,\n",
      "        0.4016, 0.3411, 0.3555, 0.4270, 0.3206, 0.5589, 0.3062, 0.3686, 0.4462,\n",
      "        0.4500, 0.4411, 0.3870, 0.4043, 0.3526, 0.2855, 0.4405, 0.4865, 0.4970,\n",
      "        0.4504, 0.3903, 0.4718, 0.4629, 0.3531, 0.4978, 0.2259, 0.4846, 0.4498,\n",
      "        0.2008, 0.4664, 0.4448, 0.4564, 0.3204, 0.3385, 0.4052, 0.3643, 0.2910,\n",
      "        0.5056, 0.2758, 0.3357, 0.4867, 0.3412, 0.3618, 0.3194, 0.3620, 0.4163,\n",
      "        0.3022, 0.4919, 0.3534, 0.4463, 0.4564, 0.2523, 0.3144, 0.3493, 0.3759,\n",
      "        0.2380, 0.4230, 0.3295, 0.3248, 0.2236, 0.4880, 0.3280, 0.4829, 0.3077,\n",
      "        0.3641, 0.4218, 0.2963, 0.3711, 0.4523, 0.3303, 0.3879, 0.2756, 0.3958,\n",
      "        0.2206, 0.2131, 0.2662, 0.3509, 0.4220, 0.2829, 0.1325, 0.1918, 0.4841,\n",
      "        0.3848, 0.4012, 0.4369, 0.3981, 0.4789, 0.3002, 0.3819, 0.2961, 0.4352,\n",
      "        0.4270, 0.4294, 0.2175, 0.2872, 0.3461, 0.4219, 0.2384, 0.1534, 0.2723,\n",
      "        0.3817, 0.2116, 0.4599, 0.4482, 0.2615, 0.4292, 0.2536, 0.4690, 0.3851,\n",
      "        0.2848, 0.5016, 0.3583, 0.3305, 0.3452, 0.2813, 0.3611, 0.1080, 0.4844,\n",
      "        0.4473, 0.5036, 0.3984, 0.3221, 0.3150, 0.4418, 0.4428, 0.3618, 0.4095,\n",
      "        0.3339, 0.3889, 0.3463, 0.3802, 0.3075, 0.3387, 0.4040, 0.3577, 0.3753,\n",
      "        0.2647, 0.3245, 0.4653, 0.4395, 0.2498, 0.3320, 0.3291, 0.3366, 0.3423,\n",
      "        0.2745, 0.3893, 0.3353, 0.3726, 0.3696, 0.3887, 0.3510, 0.2529, 0.2606,\n",
      "        0.4250, 0.4024, 0.3752, 0.4468, 0.3552, 0.3902, 0.2922, 0.3627, 0.3768,\n",
      "        0.3768, 0.2623, 0.3429, 0.2007, 0.2490, 0.3339, 0.2438, 0.2607, 0.3452,\n",
      "        0.3153, 0.3499, 0.3749, 0.3767, 0.3668, 0.4795, 0.3223, 0.4065, 0.3730,\n",
      "        0.4514, 0.4046, 0.4314, 0.3210], grad_fn=<SqueezeBackward0>)\n",
      "Epoch 2/5:\n",
      "loss tensor(0.0738, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0400, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0761, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0802, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0667, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1363, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0483, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0307, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1156, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0374, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0788, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0785, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0824, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0650, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1108, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0271, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0500, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0504, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0386, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1323, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0362, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1270, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0424, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0389, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0442, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0346, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0670, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0388, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0679, grad_fn=<MseLossBackward0>)\n",
      "label shape torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding shape torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output shape torch.Size([202])\n",
      "output tensor([0.2882, 0.2299, 0.2642, 0.2323, 0.3960, 0.2862, 0.3039, 0.3360, 0.3902,\n",
      "        0.5713, 0.4543, 0.3330, 0.4830, 0.3211, 0.3870, 0.3320, 0.3759, 0.4716,\n",
      "        0.4038, 0.3260, 0.3481, 0.4291, 0.3214, 0.5389, 0.3041, 0.3701, 0.4352,\n",
      "        0.4328, 0.4328, 0.3847, 0.3997, 0.3407, 0.2771, 0.4370, 0.4758, 0.4810,\n",
      "        0.4454, 0.3780, 0.4585, 0.4513, 0.3449, 0.4903, 0.2317, 0.4705, 0.4433,\n",
      "        0.2009, 0.4531, 0.4422, 0.4556, 0.3182, 0.3308, 0.3970, 0.3541, 0.2840,\n",
      "        0.4939, 0.2852, 0.3297, 0.4746, 0.3407, 0.3576, 0.3082, 0.3591, 0.4078,\n",
      "        0.2925, 0.4914, 0.3430, 0.4347, 0.4477, 0.2634, 0.3054, 0.3456, 0.3688,\n",
      "        0.2333, 0.4187, 0.3248, 0.3137, 0.2146, 0.4807, 0.3191, 0.4749, 0.3028,\n",
      "        0.3563, 0.4095, 0.2971, 0.3614, 0.4420, 0.3289, 0.3864, 0.2718, 0.3859,\n",
      "        0.2124, 0.2116, 0.2602, 0.3482, 0.4227, 0.2807, 0.1224, 0.1761, 0.4835,\n",
      "        0.3709, 0.3963, 0.4334, 0.3908, 0.4656, 0.2959, 0.3707, 0.2955, 0.4273,\n",
      "        0.4194, 0.4239, 0.2305, 0.2833, 0.3473, 0.4131, 0.2555, 0.1454, 0.2617,\n",
      "        0.3800, 0.2116, 0.4511, 0.4402, 0.2566, 0.4260, 0.2468, 0.4646, 0.3764,\n",
      "        0.2794, 0.4914, 0.3543, 0.3293, 0.3424, 0.2767, 0.3483, 0.1007, 0.4795,\n",
      "        0.4369, 0.4914, 0.3895, 0.3153, 0.3074, 0.4287, 0.4333, 0.3596, 0.3985,\n",
      "        0.3306, 0.3865, 0.3469, 0.3776, 0.3067, 0.3328, 0.3955, 0.3473, 0.3676,\n",
      "        0.2542, 0.3122, 0.4566, 0.4266, 0.2495, 0.3236, 0.3291, 0.3354, 0.3377,\n",
      "        0.2704, 0.3788, 0.3371, 0.3646, 0.3622, 0.3849, 0.3462, 0.2538, 0.2558,\n",
      "        0.4256, 0.4049, 0.3687, 0.4381, 0.3503, 0.3922, 0.2899, 0.3556, 0.3724,\n",
      "        0.3689, 0.2647, 0.3352, 0.2132, 0.2448, 0.3330, 0.2396, 0.2565, 0.3457,\n",
      "        0.3098, 0.3416, 0.3729, 0.3777, 0.3585, 0.4662, 0.3241, 0.3977, 0.3735,\n",
      "        0.4479, 0.4044, 0.4339, 0.3146], grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 348])) that is different to the input size (torch.Size([1, 348, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 206])) that is different to the input size (torch.Size([1, 206, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 647])) that is different to the input size (torch.Size([1, 647, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 205])) that is different to the input size (torch.Size([1, 205, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 260])) that is different to the input size (torch.Size([1, 260, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 204])) that is different to the input size (torch.Size([1, 204, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 38])) that is different to the input size (torch.Size([1, 38, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 406])) that is different to the input size (torch.Size([1, 406, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 126])) that is different to the input size (torch.Size([1, 126, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 403])) that is different to the input size (torch.Size([1, 403, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 390])) that is different to the input size (torch.Size([1, 390, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 226])) that is different to the input size (torch.Size([1, 226, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 284])) that is different to the input size (torch.Size([1, 284, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 179])) that is different to the input size (torch.Size([1, 179, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 216])) that is different to the input size (torch.Size([1, 216, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 436])) that is different to the input size (torch.Size([1, 436, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 129])) that is different to the input size (torch.Size([1, 129, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 115])) that is different to the input size (torch.Size([1, 115, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 184])) that is different to the input size (torch.Size([1, 184, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 136])) that is different to the input size (torch.Size([1, 136, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 456])) that is different to the input size (torch.Size([1, 456, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 264])) that is different to the input size (torch.Size([1, 264, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 308])) that is different to the input size (torch.Size([1, 308, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 263])) that is different to the input size (torch.Size([1, 263, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 144])) that is different to the input size (torch.Size([1, 144, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 169])) that is different to the input size (torch.Size([1, 169, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 24])) that is different to the input size (torch.Size([1, 24, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 240])) that is different to the input size (torch.Size([1, 240, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 227])) that is different to the input size (torch.Size([1, 227, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 300])) that is different to the input size (torch.Size([1, 300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 441])) that is different to the input size (torch.Size([1, 441, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:\n",
      "loss tensor(0.0712, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0393, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0357, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0830, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0471, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0362, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1305, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0634, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1069, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0481, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1128, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0438, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1261, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0442, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1218, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1153, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0260, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0519, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "label shape torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding shape torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output shape torch.Size([202])\n",
      "output tensor([0.2768, 0.2389, 0.2546, 0.2195, 0.3861, 0.2873, 0.2912, 0.3238, 0.3729,\n",
      "        0.5532, 0.4529, 0.3271, 0.4637, 0.3193, 0.3936, 0.3237, 0.3607, 0.4657,\n",
      "        0.4053, 0.3126, 0.3410, 0.4303, 0.3218, 0.5205, 0.3017, 0.3711, 0.4249,\n",
      "        0.4169, 0.4247, 0.3825, 0.3950, 0.3298, 0.2695, 0.4334, 0.4660, 0.4659,\n",
      "        0.4403, 0.3670, 0.4461, 0.4401, 0.3376, 0.4831, 0.2361, 0.4574, 0.4372,\n",
      "        0.2010, 0.4406, 0.4395, 0.4545, 0.3165, 0.3237, 0.3890, 0.3448, 0.2778,\n",
      "        0.4830, 0.2926, 0.3241, 0.4632, 0.3397, 0.3535, 0.2985, 0.3563, 0.3997,\n",
      "        0.2838, 0.4907, 0.3334, 0.4239, 0.4389, 0.2723, 0.2975, 0.3422, 0.3618,\n",
      "        0.2287, 0.4144, 0.3206, 0.3036, 0.2064, 0.4738, 0.3108, 0.4672, 0.2985,\n",
      "        0.3492, 0.3979, 0.2977, 0.3522, 0.4320, 0.3275, 0.3848, 0.2683, 0.3767,\n",
      "        0.2053, 0.2102, 0.2545, 0.3454, 0.4231, 0.2791, 0.1132, 0.1620, 0.4828,\n",
      "        0.3581, 0.3918, 0.4298, 0.3840, 0.4530, 0.2918, 0.3602, 0.2946, 0.4201,\n",
      "        0.4121, 0.4187, 0.2412, 0.2800, 0.3482, 0.4047, 0.2700, 0.1387, 0.2525,\n",
      "        0.3783, 0.2113, 0.4426, 0.4324, 0.2522, 0.4228, 0.2408, 0.4606, 0.3684,\n",
      "        0.2745, 0.4819, 0.3501, 0.3281, 0.3394, 0.2729, 0.3364, 0.0942, 0.4749,\n",
      "        0.4274, 0.4800, 0.3810, 0.3091, 0.3005, 0.4163, 0.4240, 0.3574, 0.3884,\n",
      "        0.3277, 0.3838, 0.3472, 0.3751, 0.3059, 0.3271, 0.3873, 0.3376, 0.3603,\n",
      "        0.2446, 0.3011, 0.4486, 0.4144, 0.2493, 0.3156, 0.3285, 0.3339, 0.3338,\n",
      "        0.2664, 0.3688, 0.3384, 0.3576, 0.3556, 0.3807, 0.3416, 0.2546, 0.2513,\n",
      "        0.4258, 0.4069, 0.3630, 0.4297, 0.3459, 0.3936, 0.2875, 0.3489, 0.3685,\n",
      "        0.3614, 0.2669, 0.3283, 0.2235, 0.2408, 0.3315, 0.2361, 0.2527, 0.3458,\n",
      "        0.3047, 0.3339, 0.3710, 0.3784, 0.3508, 0.4538, 0.3254, 0.3895, 0.3738,\n",
      "        0.4446, 0.4040, 0.4364, 0.3093], grad_fn=<SqueezeBackward0>)\n",
      "Epoch 4/5:\n",
      "loss tensor(0.0690, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0346, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0376, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0755, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0404, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0637, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0750, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0491, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1255, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0293, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0524, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0357, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0732, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0465, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1034, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1121, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0353, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1204, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0562, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1174, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1125, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0406, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0239, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0375, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0543, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
      "label shape torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding shape torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output shape torch.Size([202])\n",
      "output tensor([0.2672, 0.2463, 0.2460, 0.2085, 0.3772, 0.2880, 0.2800, 0.3135, 0.3576,\n",
      "        0.5368, 0.4514, 0.3220, 0.4464, 0.3176, 0.3995, 0.3165, 0.3474, 0.4603,\n",
      "        0.4069, 0.3013, 0.3347, 0.4312, 0.3225, 0.5040, 0.2996, 0.3723, 0.4158,\n",
      "        0.4028, 0.4172, 0.3808, 0.3908, 0.3203, 0.2632, 0.4302, 0.4573, 0.4522,\n",
      "        0.4356, 0.3574, 0.4349, 0.4299, 0.3316, 0.4767, 0.2398, 0.4458, 0.4320,\n",
      "        0.2017, 0.4293, 0.4373, 0.4537, 0.3158, 0.3176, 0.3820, 0.3366, 0.2727,\n",
      "        0.4732, 0.2988, 0.3194, 0.4529, 0.3389, 0.3501, 0.2903, 0.3541, 0.3922,\n",
      "        0.2764, 0.4902, 0.3250, 0.4144, 0.4308, 0.2799, 0.2908, 0.3394, 0.3554,\n",
      "        0.2250, 0.4106, 0.3172, 0.2949, 0.1994, 0.4679, 0.3037, 0.4605, 0.2951,\n",
      "        0.3432, 0.3875, 0.2984, 0.3441, 0.4228, 0.3266, 0.3836, 0.2657, 0.3686,\n",
      "        0.1996, 0.2094, 0.2495, 0.3430, 0.4235, 0.2783, 0.1053, 0.1497, 0.4823,\n",
      "        0.3468, 0.3879, 0.4265, 0.3782, 0.4416, 0.2885, 0.3507, 0.2940, 0.4139,\n",
      "        0.4056, 0.4143, 0.2505, 0.2777, 0.3494, 0.3973, 0.2829, 0.1336, 0.2449,\n",
      "        0.3769, 0.2114, 0.4348, 0.4253, 0.2488, 0.4201, 0.2360, 0.4574, 0.3615,\n",
      "        0.2704, 0.4734, 0.3464, 0.3272, 0.3366, 0.2702, 0.3257, 0.0890, 0.4709,\n",
      "        0.4191, 0.4697, 0.3735, 0.3040, 0.2946, 0.4052, 0.4156, 0.3556, 0.3795,\n",
      "        0.3255, 0.3812, 0.3477, 0.3731, 0.3058, 0.3221, 0.3801, 0.3292, 0.3539,\n",
      "        0.2363, 0.2915, 0.4416, 0.4035, 0.2498, 0.3084, 0.3279, 0.3326, 0.3310,\n",
      "        0.2630, 0.3599, 0.3399, 0.3517, 0.3502, 0.3769, 0.3377, 0.2558, 0.2476,\n",
      "        0.4261, 0.4092, 0.3583, 0.4220, 0.3424, 0.3951, 0.2854, 0.3431, 0.3653,\n",
      "        0.3547, 0.2695, 0.3223, 0.2325, 0.2375, 0.3303, 0.2336, 0.2498, 0.3463,\n",
      "        0.3004, 0.3273, 0.3697, 0.3795, 0.3440, 0.4426, 0.3269, 0.3823, 0.3744,\n",
      "        0.4419, 0.4040, 0.4392, 0.3054], grad_fn=<SqueezeBackward0>)\n",
      "Epoch 5/5:\n",
      "loss tensor(0.0671, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0514, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0545, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0410, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0361, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0353, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0984, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1004, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0253, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1116, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0693, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0429, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0341, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1150, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0520, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0279, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1136, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0452, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0400, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0327, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0694, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
      "label shape torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding shape torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output shape torch.Size([202])\n",
      "output tensor([0.2591, 0.2524, 0.2383, 0.1992, 0.3691, 0.2883, 0.2701, 0.3048, 0.3439,\n",
      "        0.5221, 0.4499, 0.3176, 0.4309, 0.3160, 0.4048, 0.3105, 0.3355, 0.4551,\n",
      "        0.4084, 0.2917, 0.3290, 0.4318, 0.3233, 0.4892, 0.2977, 0.3736, 0.4077,\n",
      "        0.3903, 0.4103, 0.3796, 0.3870, 0.3120, 0.2579, 0.4274, 0.4496, 0.4397,\n",
      "        0.4314, 0.3492, 0.4250, 0.4204, 0.3268, 0.4710, 0.2429, 0.4354, 0.4276,\n",
      "        0.2029, 0.4191, 0.4356, 0.4530, 0.3158, 0.3123, 0.3757, 0.3295, 0.2685,\n",
      "        0.4643, 0.3041, 0.3155, 0.4437, 0.3382, 0.3472, 0.2836, 0.3524, 0.3855,\n",
      "        0.2701, 0.4901, 0.3175, 0.4059, 0.4233, 0.2863, 0.2853, 0.3372, 0.3495,\n",
      "        0.2219, 0.4072, 0.3145, 0.2874, 0.1934, 0.4627, 0.2975, 0.4545, 0.2924,\n",
      "        0.3381, 0.3781, 0.2992, 0.3369, 0.4145, 0.3261, 0.3827, 0.2639, 0.3614,\n",
      "        0.1950, 0.2091, 0.2451, 0.3411, 0.4242, 0.2783, 0.0985, 0.1391, 0.4820,\n",
      "        0.3369, 0.3848, 0.4234, 0.3732, 0.4312, 0.2860, 0.3422, 0.2936, 0.4086,\n",
      "        0.3998, 0.4105, 0.2586, 0.2763, 0.3507, 0.3906, 0.2943, 0.1298, 0.2386,\n",
      "        0.3759, 0.2117, 0.4278, 0.4189, 0.2464, 0.4179, 0.2321, 0.4548, 0.3556,\n",
      "        0.2672, 0.4659, 0.3431, 0.3268, 0.3342, 0.2684, 0.3162, 0.0849, 0.4676,\n",
      "        0.4120, 0.4604, 0.3668, 0.2998, 0.2896, 0.3952, 0.4079, 0.3542, 0.3718,\n",
      "        0.3240, 0.3788, 0.3485, 0.3717, 0.3061, 0.3177, 0.3737, 0.3218, 0.3482,\n",
      "        0.2291, 0.2833, 0.4355, 0.3937, 0.2508, 0.3020, 0.3273, 0.3315, 0.3291,\n",
      "        0.2601, 0.3520, 0.3415, 0.3469, 0.3457, 0.3733, 0.3344, 0.2574, 0.2447,\n",
      "        0.4266, 0.4115, 0.3545, 0.4150, 0.3396, 0.3965, 0.2837, 0.3380, 0.3629,\n",
      "        0.3487, 0.2723, 0.3173, 0.2402, 0.2348, 0.3291, 0.2320, 0.2475, 0.3469,\n",
      "        0.2969, 0.3216, 0.3687, 0.3809, 0.3380, 0.4326, 0.3284, 0.3759, 0.3751,\n",
      "        0.4397, 0.4044, 0.4424, 0.3026], grad_fn=<SqueezeBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fonction pour obtenir les embeddings correspondant aux squences\n",
    "def get_embeddings(sequences, embeddings_dict):\n",
    "    embeddings = []\n",
    "    for sequence_id in sequences:\n",
    "        embedding = embeddings_dict[sequence_id]\n",
    "        print(embedding)\n",
    "        embeddings.append(embedding)\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding(sequence_id, embeddings_dict):\n",
    "    return embeddings_dict[sequence_id]\n",
    "\n",
    "\n",
    "# Fonction pour entraner le modle en utilisant la descente de gradient stochastique (SGD)\n",
    "def train_model_stochastic(model, optimizer, loss_fn, sequences, conservation_scores):\n",
    "    model.train()\n",
    "    for i in range(len(sequences)):\n",
    "        sequence_id = sequences[i]\n",
    "        if i == len(sequences) - 1:\n",
    "            print(\"sequence_id\", sequence_id)\n",
    "            print(\"embedding\", embedding)\n",
    "        embedding = get_embedding(sequence_id, embeddings_dict)\n",
    "        #embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        label = torch.tensor(conservation_scores[i], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(embedding).squeeze()\n",
    "\n",
    "        \n",
    "        loss = loss_fn(output, label)\n",
    "        print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i == len(sequences) - 1 or False:\n",
    "            print(\"label shape\", label.shape)\n",
    "            print(\"label\", label)\n",
    "            print(\"embedding shape\", embedding.shape)\n",
    "            print(\"embedding\", embedding)\n",
    "            print(\"output shape\", output.shape)\n",
    "            print(\"output\", output)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# Dfinir les modles de rgression linaire, Multi layer perceptron\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# valuation du modle sur l'ensemble de validation\n",
    "def evaluate_model(model, loss_fn, data_loader):\n",
    "    running_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "# Configuration des hyperparamtres\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# Crer l'ensemble de donnes\n",
    "dataset = [(embeddings_dict[sequence], conservation_scores) for sequence, conservation_scores in zip(sequences, conservation_scores_tensors)]\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Crer les data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialiser le modle, la fonction de perte et l'optimiseur\n",
    "model = LinearRegression(input_size=320)\n",
    "#model = MLP(input_size=320, hidden_size=64, output_size=1)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_losses = []\n",
    "# Entranement du modle\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "    model.train()\n",
    "    train_model_stochastic(model, optimizer, loss_fn,\n",
    "                           sequences[:100], conservation_scores_tensors[:100])\n",
    "\n",
    "    # Validation du modle\n",
    "    model.eval()\n",
    "    val_loss = evaluate_model(model, loss_fn, val_loader)\n",
    "    val_losses.append(val_loss) \n",
    "    #print(f'Validation Loss: {val_loss}')\n",
    "# on trace la perte de validation au fil des poques\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345f4348-f648-4199-9e54-f67964b2343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1827,  0.4544,  0.0703,  ...,  0.9286, -0.1682,  0.0573],\n",
      "        [-0.0597, -0.0637,  0.7307,  ...,  0.3019,  0.2616,  0.5997],\n",
      "        [ 0.1731, -0.0389,  0.8230,  ...,  0.5839, -0.2457,  0.3610],\n",
      "        ...,\n",
      "        [-0.0797,  0.2541,  0.7844,  ...,  0.5837,  0.1552,  0.1666],\n",
      "        [ 0.2382, -0.3610,  0.6018,  ..., -0.0876, -0.3903,  0.0103],\n",
      "        [-0.0087, -0.5144,  0.2969,  ..., -0.0461, -0.0331,  0.4260]])\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo .pt\n",
    "embedding = torch.load('curated_dataset/example_embeddings_esm2_reduced_input/A0A1I4YI28.1/1-164.pt')\n",
    "\n",
    "# Ahora puedes acceder al tensor del embedding\n",
    "print(embedding[\"representations\"][6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f806362-69bc-477d-8d1a-841b088632e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
