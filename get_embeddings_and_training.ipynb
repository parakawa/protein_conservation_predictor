{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "445c286e-199c-4bda-a99c-b5452e873523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "557bffc9-de2e-4c87-9f2f-a1195cd0615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_dict len torch.Size([59, 320])\n",
      "embeddings_dict tensor([[-0.6685, -0.0708, -0.5033,  ...,  0.8954,  0.4106, -0.5340],\n",
      "        [-0.6899, -0.1052,  0.0013,  ...,  0.5112, -0.0517, -0.3438],\n",
      "        [-0.5972, -0.1812,  0.3029,  ...,  0.4339,  0.3076, -0.0928],\n",
      "        ...,\n",
      "        [-0.2558, -0.0050,  0.2232,  ...,  0.4575, -0.0603, -0.1484],\n",
      "        [-0.3865, -0.1068,  0.3723,  ...,  0.4398, -0.3131, -0.0656],\n",
      "        [-0.1832,  0.3298,  0.3563,  ..., -0.0673, -0.4823, -0.1598]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings_vectors_curated_data(folder_path):\n",
    "    # Initialize a list to store the vectors\n",
    "    embeddings = {}\n",
    "\n",
    "    # Traverse through each folder in the specified directory\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a .pt file\n",
    "            if file.endswith('.pt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                embedding = torch.load(file_path)\n",
    "                embeddings[embedding[\"label\"]] = embedding[\"representations\"][6]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings_dict = get_embeddings_vectors_curated_data('curated_dataset/example_embeddings_esm2_reduced_input')\n",
    "print(\"embeddings_dict len\", (embeddings_dict[\"A0A1X7AIY7.1/282-340\"]).shape)\n",
    "print(\"embeddings_dict\", embeddings_dict[\"A0A1X7AIY7.1/282-340\"])\n",
    "len(\"STPIRIFANGRRRVEVLRDNRLIYATSVNAGSQEIDTSSFPQGSYQLTIRIFNGSTLEQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7663a67f-94c0-41de-9de1-81014ea69328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('QIGGEDKADIAPILKEGFVGPGMQINNLLQERGEIVATVICGDNYFNENLDEATDTILGMIGQFNPDIVIAGPSFNAGRYGMACGAVCKAVSEKFNIPTLTGMYIESPGVDGYRKYTYIVETANSAVGMRTALPAMVKLALKLVDGVELGDPKEEGYIARGVRRNYFHAVRGSKRAVDMLIAKINDQPFTTEYPMPTFDRVAPNPHIVDMSKATIALVTSGGIVPKGNPDHIESSSASKFGKYDIEGFTNLTEKTHETAHGGYDPVYANLDADRVLPVDVLRELEAEGVIGKLHRYFYTTVGNGTSVANAKKFAAAIGKELVEAKVDAVILTST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0f1b79-a98e-445a-970b-de457463f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conservation_scores(csv_file):\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, delimiter=',', names=[\n",
    "                     'sequence id', 'conservation score'], header=0)\n",
    "\n",
    "    sequences = df['sequence id'].values\n",
    "    conservation_scores = df['conservation score'].apply(lambda x: np.array(\n",
    "    [float(i) if i != 'nan' else 0.0 for i in x.split()], dtype=np.float32)).values\n",
    "    #print(\"conservation_scores\", conservation_scores)\n",
    "\n",
    "    return sequences, conservation_scores\n",
    "\n",
    "sequences, conservation_scores_tensors = get_conservation_scores('curated_dataset/reduced_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadad60d-9d2f-4eed-b4cd-20f4b8d57b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conservation_scores_tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51ef0555-34a2-4492-892d-86df94a89cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "loss tensor(0.3399, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0966, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1242, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1548, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1613, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1825, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1934, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1061, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1238, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0702, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0588, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1211, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0741, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0735, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.2016, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1089, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0871, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0852, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0358, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0380, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0617, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0387, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1488, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0876, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0763, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0498, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0793, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0450, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0404, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1098, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0647, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
      "label torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output torch.Size([202, 1])\n",
      "output tensor([[0.6956],\n",
      "        [0.3680],\n",
      "        [0.4197],\n",
      "        [0.4557],\n",
      "        [0.3482],\n",
      "        [0.2215],\n",
      "        [0.5337],\n",
      "        [0.2609],\n",
      "        [0.3322],\n",
      "        [0.4445],\n",
      "        [0.4028],\n",
      "        [0.3787],\n",
      "        [0.2041],\n",
      "        [0.2444],\n",
      "        [0.4072],\n",
      "        [0.3903],\n",
      "        [0.2961],\n",
      "        [0.5401],\n",
      "        [0.4888],\n",
      "        [0.4118],\n",
      "        [0.2080],\n",
      "        [0.4462],\n",
      "        [0.4084],\n",
      "        [0.4415],\n",
      "        [0.3303],\n",
      "        [0.4066],\n",
      "        [0.5085],\n",
      "        [0.3459],\n",
      "        [0.3372],\n",
      "        [0.4064],\n",
      "        [0.1983],\n",
      "        [0.4337],\n",
      "        [0.4234],\n",
      "        [0.3329],\n",
      "        [0.3995],\n",
      "        [0.1988],\n",
      "        [0.3181],\n",
      "        [0.3784],\n",
      "        [0.3615],\n",
      "        [0.3140],\n",
      "        [0.3876],\n",
      "        [0.3865],\n",
      "        [0.2924],\n",
      "        [0.5379],\n",
      "        [0.4032],\n",
      "        [0.4135],\n",
      "        [0.3805],\n",
      "        [0.4472],\n",
      "        [0.4418],\n",
      "        [0.4840],\n",
      "        [0.4254],\n",
      "        [0.2436],\n",
      "        [0.4551],\n",
      "        [0.4018],\n",
      "        [0.4941],\n",
      "        [0.3589],\n",
      "        [0.3425],\n",
      "        [0.3354],\n",
      "        [0.1941],\n",
      "        [0.3849],\n",
      "        [0.3228],\n",
      "        [0.3531],\n",
      "        [0.2841],\n",
      "        [0.4206],\n",
      "        [0.4842],\n",
      "        [0.3823],\n",
      "        [0.4051],\n",
      "        [0.3030],\n",
      "        [0.3269],\n",
      "        [0.4643],\n",
      "        [0.4188],\n",
      "        [0.3763],\n",
      "        [0.3915],\n",
      "        [0.4375],\n",
      "        [0.3878],\n",
      "        [0.4659],\n",
      "        [0.3978],\n",
      "        [0.4158],\n",
      "        [0.4411],\n",
      "        [0.3993],\n",
      "        [0.3608],\n",
      "        [0.4492],\n",
      "        [0.3115],\n",
      "        [0.3720],\n",
      "        [0.3873],\n",
      "        [0.3894],\n",
      "        [0.4440],\n",
      "        [0.4373],\n",
      "        [0.4060],\n",
      "        [0.5570],\n",
      "        [0.4485],\n",
      "        [0.5180],\n",
      "        [0.4511],\n",
      "        [0.4300],\n",
      "        [0.4004],\n",
      "        [0.4604],\n",
      "        [0.4058],\n",
      "        [0.1588],\n",
      "        [0.2239],\n",
      "        [0.1105],\n",
      "        [0.3359],\n",
      "        [0.4724],\n",
      "        [0.4039],\n",
      "        [0.3187],\n",
      "        [0.4578],\n",
      "        [0.2329],\n",
      "        [0.3995],\n",
      "        [0.4738],\n",
      "        [0.5305],\n",
      "        [0.4313],\n",
      "        [0.3801],\n",
      "        [0.4939],\n",
      "        [0.4412],\n",
      "        [0.3556],\n",
      "        [0.2816],\n",
      "        [0.5191],\n",
      "        [0.3609],\n",
      "        [0.3805],\n",
      "        [0.3305],\n",
      "        [0.3083],\n",
      "        [0.3935],\n",
      "        [0.4337],\n",
      "        [0.4239],\n",
      "        [0.4442],\n",
      "        [0.4887],\n",
      "        [0.4448],\n",
      "        [0.4915],\n",
      "        [0.4586],\n",
      "        [0.3332],\n",
      "        [0.4790],\n",
      "        [0.3038],\n",
      "        [0.4477],\n",
      "        [0.4442],\n",
      "        [0.4145],\n",
      "        [0.3697],\n",
      "        [0.3693],\n",
      "        [0.3481],\n",
      "        [0.3532],\n",
      "        [0.2812],\n",
      "        [0.3913],\n",
      "        [0.3045],\n",
      "        [0.1962],\n",
      "        [0.3951],\n",
      "        [0.4214],\n",
      "        [0.4520],\n",
      "        [0.3181],\n",
      "        [0.4119],\n",
      "        [0.4774],\n",
      "        [0.4036],\n",
      "        [0.3420],\n",
      "        [0.3824],\n",
      "        [0.5594],\n",
      "        [0.3031],\n",
      "        [0.3907],\n",
      "        [0.5017],\n",
      "        [0.3911],\n",
      "        [0.4184],\n",
      "        [0.4095],\n",
      "        [0.3070],\n",
      "        [0.3001],\n",
      "        [0.2842],\n",
      "        [0.4342],\n",
      "        [0.4147],\n",
      "        [0.2563],\n",
      "        [0.3475],\n",
      "        [0.5038],\n",
      "        [0.4604],\n",
      "        [0.3149],\n",
      "        [0.3119],\n",
      "        [0.4197],\n",
      "        [0.4416],\n",
      "        [0.4597],\n",
      "        [0.4805],\n",
      "        [0.4838],\n",
      "        [0.4105],\n",
      "        [0.3545],\n",
      "        [0.3734],\n",
      "        [0.3635],\n",
      "        [0.4145],\n",
      "        [0.4113],\n",
      "        [0.2522],\n",
      "        [0.3779],\n",
      "        [0.4760],\n",
      "        [0.3388],\n",
      "        [0.3455],\n",
      "        [0.3520],\n",
      "        [0.4726],\n",
      "        [0.5346],\n",
      "        [0.3691],\n",
      "        [0.4871],\n",
      "        [0.4548],\n",
      "        [0.3793],\n",
      "        [0.4711],\n",
      "        [0.3805],\n",
      "        [0.2893],\n",
      "        [0.3323],\n",
      "        [0.3556],\n",
      "        [0.3607],\n",
      "        [0.4296],\n",
      "        [0.4019],\n",
      "        [0.4474],\n",
      "        [0.4858]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 2/5:\n",
      "loss tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0226, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 160])) that is different to the input size (torch.Size([1, 160, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 95])) that is different to the input size (torch.Size([1, 95, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 200])) that is different to the input size (torch.Size([1, 200, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 103])) that is different to the input size (torch.Size([1, 103, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 27])) that is different to the input size (torch.Size([1, 27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 497])) that is different to the input size (torch.Size([1, 497, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 93])) that is different to the input size (torch.Size([1, 93, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 400])) that is different to the input size (torch.Size([1, 400, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 137])) that is different to the input size (torch.Size([1, 137, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 39])) that is different to the input size (torch.Size([1, 39, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 84])) that is different to the input size (torch.Size([1, 84, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 235])) that is different to the input size (torch.Size([1, 235, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 81])) that is different to the input size (torch.Size([1, 81, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 364])) that is different to the input size (torch.Size([1, 364, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 236])) that is different to the input size (torch.Size([1, 236, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 61])) that is different to the input size (torch.Size([1, 61, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 417])) that is different to the input size (torch.Size([1, 417, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 197])) that is different to the input size (torch.Size([1, 197, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 97])) that is different to the input size (torch.Size([1, 97, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 386])) that is different to the input size (torch.Size([1, 386, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 104])) that is different to the input size (torch.Size([1, 104, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 627])) that is different to the input size (torch.Size([1, 627, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 194])) that is different to the input size (torch.Size([1, 194, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 300])) that is different to the input size (torch.Size([1, 300, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 374])) that is different to the input size (torch.Size([1, 374, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 185])) that is different to the input size (torch.Size([1, 185, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 182])) that is different to the input size (torch.Size([1, 182, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 270])) that is different to the input size (torch.Size([1, 270, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 193])) that is different to the input size (torch.Size([1, 193, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 226])) that is different to the input size (torch.Size([1, 226, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 178])) that is different to the input size (torch.Size([1, 178, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 346])) that is different to the input size (torch.Size([1, 346, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 192])) that is different to the input size (torch.Size([1, 192, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 289])) that is different to the input size (torch.Size([1, 289, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 268])) that is different to the input size (torch.Size([1, 268, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 221])) that is different to the input size (torch.Size([1, 221, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 292])) that is different to the input size (torch.Size([1, 292, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 473])) that is different to the input size (torch.Size([1, 473, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 265])) that is different to the input size (torch.Size([1, 265, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 76])) that is different to the input size (torch.Size([1, 76, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 184])) that is different to the input size (torch.Size([1, 184, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 227])) that is different to the input size (torch.Size([1, 227, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 79])) that is different to the input size (torch.Size([1, 79, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 215])) that is different to the input size (torch.Size([1, 215, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 291])) that is different to the input size (torch.Size([1, 291, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 218])) that is different to the input size (torch.Size([1, 218, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 62])) that is different to the input size (torch.Size([1, 62, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 321])) that is different to the input size (torch.Size([1, 321, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 163])) that is different to the input size (torch.Size([1, 163, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 153])) that is different to the input size (torch.Size([1, 153, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 195])) that is different to the input size (torch.Size([1, 195, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 21])) that is different to the input size (torch.Size([1, 21, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 37])) that is different to the input size (torch.Size([1, 37, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 362])) that is different to the input size (torch.Size([1, 362, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 303])) that is different to the input size (torch.Size([1, 303, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 372])) that is different to the input size (torch.Size([1, 372, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 349])) that is different to the input size (torch.Size([1, 349, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 378])) that is different to the input size (torch.Size([1, 378, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/patyarakawa/mi_entorno/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1, 286])) that is different to the input size (torch.Size([1, 286, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0353, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0274, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0578, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0455, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1115, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0423, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0624, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0705, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1826, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0780, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1064, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0421, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1426, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0290, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0799, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0658, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0641, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0369, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0965, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0384, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0285, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0704, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0440, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0701, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0379, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0574, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0623, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
      "label torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output torch.Size([202, 1])\n",
      "output tensor([[0.6706],\n",
      "        [0.3683],\n",
      "        [0.4114],\n",
      "        [0.4379],\n",
      "        [0.3387],\n",
      "        [0.2311],\n",
      "        [0.5203],\n",
      "        [0.2498],\n",
      "        [0.3181],\n",
      "        [0.4310],\n",
      "        [0.4037],\n",
      "        [0.3708],\n",
      "        [0.1961],\n",
      "        [0.2430],\n",
      "        [0.4109],\n",
      "        [0.3804],\n",
      "        [0.2854],\n",
      "        [0.5340],\n",
      "        [0.4881],\n",
      "        [0.3907],\n",
      "        [0.2060],\n",
      "        [0.4479],\n",
      "        [0.4065],\n",
      "        [0.4252],\n",
      "        [0.3305],\n",
      "        [0.4059],\n",
      "        [0.4979],\n",
      "        [0.3383],\n",
      "        [0.3356],\n",
      "        [0.4059],\n",
      "        [0.2039],\n",
      "        [0.4218],\n",
      "        [0.4154],\n",
      "        [0.3349],\n",
      "        [0.3899],\n",
      "        [0.2002],\n",
      "        [0.3161],\n",
      "        [0.3684],\n",
      "        [0.3532],\n",
      "        [0.3096],\n",
      "        [0.3818],\n",
      "        [0.3835],\n",
      "        [0.2898],\n",
      "        [0.5273],\n",
      "        [0.3998],\n",
      "        [0.4116],\n",
      "        [0.3757],\n",
      "        [0.4472],\n",
      "        [0.4424],\n",
      "        [0.4823],\n",
      "        [0.4204],\n",
      "        [0.2471],\n",
      "        [0.4473],\n",
      "        [0.3951],\n",
      "        [0.4864],\n",
      "        [0.3594],\n",
      "        [0.3420],\n",
      "        [0.3312],\n",
      "        [0.2014],\n",
      "        [0.3806],\n",
      "        [0.3074],\n",
      "        [0.3565],\n",
      "        [0.2870],\n",
      "        [0.4098],\n",
      "        [0.4839],\n",
      "        [0.3747],\n",
      "        [0.3950],\n",
      "        [0.3008],\n",
      "        [0.3273],\n",
      "        [0.4484],\n",
      "        [0.4143],\n",
      "        [0.3793],\n",
      "        [0.3844],\n",
      "        [0.4357],\n",
      "        [0.3833],\n",
      "        [0.4484],\n",
      "        [0.3860],\n",
      "        [0.4121],\n",
      "        [0.4322],\n",
      "        [0.3952],\n",
      "        [0.3569],\n",
      "        [0.4430],\n",
      "        [0.3110],\n",
      "        [0.3765],\n",
      "        [0.3818],\n",
      "        [0.3872],\n",
      "        [0.4440],\n",
      "        [0.4375],\n",
      "        [0.4037],\n",
      "        [0.5487],\n",
      "        [0.4346],\n",
      "        [0.5134],\n",
      "        [0.4480],\n",
      "        [0.4294],\n",
      "        [0.4018],\n",
      "        [0.4581],\n",
      "        [0.3946],\n",
      "        [0.1487],\n",
      "        [0.2336],\n",
      "        [0.1022],\n",
      "        [0.3315],\n",
      "        [0.4675],\n",
      "        [0.3997],\n",
      "        [0.3139],\n",
      "        [0.4510],\n",
      "        [0.2355],\n",
      "        [0.4048],\n",
      "        [0.4697],\n",
      "        [0.5247],\n",
      "        [0.4306],\n",
      "        [0.3834],\n",
      "        [0.4819],\n",
      "        [0.4422],\n",
      "        [0.3567],\n",
      "        [0.2866],\n",
      "        [0.5100],\n",
      "        [0.3544],\n",
      "        [0.3881],\n",
      "        [0.3293],\n",
      "        [0.3107],\n",
      "        [0.3915],\n",
      "        [0.4269],\n",
      "        [0.4229],\n",
      "        [0.4364],\n",
      "        [0.4887],\n",
      "        [0.4400],\n",
      "        [0.4854],\n",
      "        [0.4524],\n",
      "        [0.3401],\n",
      "        [0.4792],\n",
      "        [0.3106],\n",
      "        [0.4439],\n",
      "        [0.4371],\n",
      "        [0.4036],\n",
      "        [0.3754],\n",
      "        [0.3642],\n",
      "        [0.3428],\n",
      "        [0.3476],\n",
      "        [0.2772],\n",
      "        [0.3832],\n",
      "        [0.3038],\n",
      "        [0.1995],\n",
      "        [0.3983],\n",
      "        [0.4171],\n",
      "        [0.4499],\n",
      "        [0.3255],\n",
      "        [0.4139],\n",
      "        [0.4768],\n",
      "        [0.4042],\n",
      "        [0.3455],\n",
      "        [0.3805],\n",
      "        [0.5502],\n",
      "        [0.3066],\n",
      "        [0.3828],\n",
      "        [0.4904],\n",
      "        [0.3876],\n",
      "        [0.4109],\n",
      "        [0.4068],\n",
      "        [0.3088],\n",
      "        [0.3083],\n",
      "        [0.2910],\n",
      "        [0.4220],\n",
      "        [0.4120],\n",
      "        [0.2586],\n",
      "        [0.3548],\n",
      "        [0.4898],\n",
      "        [0.4479],\n",
      "        [0.3210],\n",
      "        [0.3127],\n",
      "        [0.4187],\n",
      "        [0.4337],\n",
      "        [0.4649],\n",
      "        [0.4830],\n",
      "        [0.4716],\n",
      "        [0.4079],\n",
      "        [0.3511],\n",
      "        [0.3795],\n",
      "        [0.3632],\n",
      "        [0.4077],\n",
      "        [0.4024],\n",
      "        [0.2547],\n",
      "        [0.3786],\n",
      "        [0.4678],\n",
      "        [0.3416],\n",
      "        [0.3496],\n",
      "        [0.3560],\n",
      "        [0.4660],\n",
      "        [0.5254],\n",
      "        [0.3700],\n",
      "        [0.4796],\n",
      "        [0.4463],\n",
      "        [0.3762],\n",
      "        [0.4711],\n",
      "        [0.3746],\n",
      "        [0.2886],\n",
      "        [0.3380],\n",
      "        [0.3574],\n",
      "        [0.3662],\n",
      "        [0.4298],\n",
      "        [0.4068],\n",
      "        [0.4506],\n",
      "        [0.4707]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 3/5:\n",
      "loss tensor(0.0784, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0336, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0821, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0549, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0511, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0724, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0343, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0786, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1733, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1032, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0777, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0357, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0362, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1370, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0283, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0627, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0716, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0944, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0442, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0462, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0633, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0330, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0640, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0496, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0998, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0406, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0573, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0736, grad_fn=<MseLossBackward0>)\n",
      "label torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output torch.Size([202, 1])\n",
      "output tensor([[0.6472],\n",
      "        [0.3674],\n",
      "        [0.4028],\n",
      "        [0.4211],\n",
      "        [0.3294],\n",
      "        [0.2388],\n",
      "        [0.5074],\n",
      "        [0.2396],\n",
      "        [0.3049],\n",
      "        [0.4180],\n",
      "        [0.4034],\n",
      "        [0.3631],\n",
      "        [0.1885],\n",
      "        [0.2409],\n",
      "        [0.4132],\n",
      "        [0.3708],\n",
      "        [0.2754],\n",
      "        [0.5273],\n",
      "        [0.4865],\n",
      "        [0.3713],\n",
      "        [0.2036],\n",
      "        [0.4484],\n",
      "        [0.4039],\n",
      "        [0.4096],\n",
      "        [0.3300],\n",
      "        [0.4044],\n",
      "        [0.4873],\n",
      "        [0.3309],\n",
      "        [0.3332],\n",
      "        [0.4049],\n",
      "        [0.2083],\n",
      "        [0.4103],\n",
      "        [0.4076],\n",
      "        [0.3357],\n",
      "        [0.3806],\n",
      "        [0.2008],\n",
      "        [0.3133],\n",
      "        [0.3588],\n",
      "        [0.3450],\n",
      "        [0.3047],\n",
      "        [0.3762],\n",
      "        [0.3800],\n",
      "        [0.2865],\n",
      "        [0.5168],\n",
      "        [0.3960],\n",
      "        [0.4094],\n",
      "        [0.3705],\n",
      "        [0.4465],\n",
      "        [0.4420],\n",
      "        [0.4803],\n",
      "        [0.4151],\n",
      "        [0.2497],\n",
      "        [0.4394],\n",
      "        [0.3884],\n",
      "        [0.4786],\n",
      "        [0.3587],\n",
      "        [0.3410],\n",
      "        [0.3266],\n",
      "        [0.2073],\n",
      "        [0.3759],\n",
      "        [0.2931],\n",
      "        [0.3587],\n",
      "        [0.2889],\n",
      "        [0.3993],\n",
      "        [0.4829],\n",
      "        [0.3671],\n",
      "        [0.3851],\n",
      "        [0.2978],\n",
      "        [0.3265],\n",
      "        [0.4334],\n",
      "        [0.4096],\n",
      "        [0.3811],\n",
      "        [0.3770],\n",
      "        [0.4331],\n",
      "        [0.3786],\n",
      "        [0.4318],\n",
      "        [0.3744],\n",
      "        [0.4081],\n",
      "        [0.4232],\n",
      "        [0.3908],\n",
      "        [0.3528],\n",
      "        [0.4367],\n",
      "        [0.3097],\n",
      "        [0.3796],\n",
      "        [0.3761],\n",
      "        [0.3842],\n",
      "        [0.4434],\n",
      "        [0.4369],\n",
      "        [0.4011],\n",
      "        [0.5403],\n",
      "        [0.4214],\n",
      "        [0.5083],\n",
      "        [0.4444],\n",
      "        [0.4279],\n",
      "        [0.4024],\n",
      "        [0.4557],\n",
      "        [0.3837],\n",
      "        [0.1394],\n",
      "        [0.2415],\n",
      "        [0.0945],\n",
      "        [0.3269],\n",
      "        [0.4618],\n",
      "        [0.3952],\n",
      "        [0.3087],\n",
      "        [0.4440],\n",
      "        [0.2373],\n",
      "        [0.4085],\n",
      "        [0.4653],\n",
      "        [0.5184],\n",
      "        [0.4293],\n",
      "        [0.3852],\n",
      "        [0.4705],\n",
      "        [0.4423],\n",
      "        [0.3569],\n",
      "        [0.2901],\n",
      "        [0.5014],\n",
      "        [0.3482],\n",
      "        [0.3940],\n",
      "        [0.3276],\n",
      "        [0.3121],\n",
      "        [0.3887],\n",
      "        [0.4201],\n",
      "        [0.4211],\n",
      "        [0.4286],\n",
      "        [0.4882],\n",
      "        [0.4350],\n",
      "        [0.4790],\n",
      "        [0.4459],\n",
      "        [0.3455],\n",
      "        [0.4786],\n",
      "        [0.3160],\n",
      "        [0.4401],\n",
      "        [0.4298],\n",
      "        [0.3930],\n",
      "        [0.3797],\n",
      "        [0.3589],\n",
      "        [0.3372],\n",
      "        [0.3418],\n",
      "        [0.2731],\n",
      "        [0.3751],\n",
      "        [0.3024],\n",
      "        [0.2017],\n",
      "        [0.4002],\n",
      "        [0.4127],\n",
      "        [0.4474],\n",
      "        [0.3314],\n",
      "        [0.4149],\n",
      "        [0.4755],\n",
      "        [0.4042],\n",
      "        [0.3480],\n",
      "        [0.3778],\n",
      "        [0.5409],\n",
      "        [0.3091],\n",
      "        [0.3748],\n",
      "        [0.4793],\n",
      "        [0.3838],\n",
      "        [0.4033],\n",
      "        [0.4038],\n",
      "        [0.3097],\n",
      "        [0.3148],\n",
      "        [0.2963],\n",
      "        [0.4105],\n",
      "        [0.4088],\n",
      "        [0.2600],\n",
      "        [0.3603],\n",
      "        [0.4765],\n",
      "        [0.4360],\n",
      "        [0.3256],\n",
      "        [0.3128],\n",
      "        [0.4171],\n",
      "        [0.4256],\n",
      "        [0.4686],\n",
      "        [0.4846],\n",
      "        [0.4601],\n",
      "        [0.4047],\n",
      "        [0.3473],\n",
      "        [0.3839],\n",
      "        [0.3621],\n",
      "        [0.4006],\n",
      "        [0.3938],\n",
      "        [0.2563],\n",
      "        [0.3787],\n",
      "        [0.4597],\n",
      "        [0.3429],\n",
      "        [0.3526],\n",
      "        [0.3588],\n",
      "        [0.4594],\n",
      "        [0.5161],\n",
      "        [0.3701],\n",
      "        [0.4720],\n",
      "        [0.4378],\n",
      "        [0.3725],\n",
      "        [0.4704],\n",
      "        [0.3685],\n",
      "        [0.2873],\n",
      "        [0.3422],\n",
      "        [0.3585],\n",
      "        [0.3703],\n",
      "        [0.4293],\n",
      "        [0.4104],\n",
      "        [0.4531],\n",
      "        [0.4566]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 4/5:\n",
      "loss tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0828, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0470, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0406, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0772, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0366, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0302, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0560, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0335, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0411, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0688, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1045, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0561, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1648, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0709, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1003, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0400, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0580, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1318, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0277, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0525, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0923, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0427, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0529, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0271, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0587, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0654, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0270, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0386, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0354, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0955, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0594, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0722, grad_fn=<MseLossBackward0>)\n",
      "label torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output torch.Size([202, 1])\n",
      "output tensor([[0.6260],\n",
      "        [0.3663],\n",
      "        [0.3950],\n",
      "        [0.4060],\n",
      "        [0.3212],\n",
      "        [0.2456],\n",
      "        [0.4957],\n",
      "        [0.2310],\n",
      "        [0.2932],\n",
      "        [0.4063],\n",
      "        [0.4030],\n",
      "        [0.3562],\n",
      "        [0.1820],\n",
      "        [0.2391],\n",
      "        [0.4150],\n",
      "        [0.3625],\n",
      "        [0.2669],\n",
      "        [0.5209],\n",
      "        [0.4851],\n",
      "        [0.3542],\n",
      "        [0.2017],\n",
      "        [0.4487],\n",
      "        [0.4016],\n",
      "        [0.3956],\n",
      "        [0.3297],\n",
      "        [0.4030],\n",
      "        [0.4776],\n",
      "        [0.3245],\n",
      "        [0.3310],\n",
      "        [0.4042],\n",
      "        [0.2125],\n",
      "        [0.4002],\n",
      "        [0.4007],\n",
      "        [0.3363],\n",
      "        [0.3724],\n",
      "        [0.2016],\n",
      "        [0.3107],\n",
      "        [0.3505],\n",
      "        [0.3378],\n",
      "        [0.3001],\n",
      "        [0.3714],\n",
      "        [0.3770],\n",
      "        [0.2835],\n",
      "        [0.5072],\n",
      "        [0.3928],\n",
      "        [0.4076],\n",
      "        [0.3659],\n",
      "        [0.4460],\n",
      "        [0.4417],\n",
      "        [0.4789],\n",
      "        [0.4106],\n",
      "        [0.2522],\n",
      "        [0.4323],\n",
      "        [0.3825],\n",
      "        [0.4716],\n",
      "        [0.3579],\n",
      "        [0.3403],\n",
      "        [0.3226],\n",
      "        [0.2129],\n",
      "        [0.3719],\n",
      "        [0.2807],\n",
      "        [0.3607],\n",
      "        [0.2907],\n",
      "        [0.3899],\n",
      "        [0.4821],\n",
      "        [0.3604],\n",
      "        [0.3762],\n",
      "        [0.2949],\n",
      "        [0.3255],\n",
      "        [0.4200],\n",
      "        [0.4054],\n",
      "        [0.3828],\n",
      "        [0.3701],\n",
      "        [0.4308],\n",
      "        [0.3746],\n",
      "        [0.4169],\n",
      "        [0.3639],\n",
      "        [0.4046],\n",
      "        [0.4150],\n",
      "        [0.3869],\n",
      "        [0.3493],\n",
      "        [0.4311],\n",
      "        [0.3087],\n",
      "        [0.3822],\n",
      "        [0.3712],\n",
      "        [0.3814],\n",
      "        [0.4430],\n",
      "        [0.4365],\n",
      "        [0.3989],\n",
      "        [0.5325],\n",
      "        [0.4098],\n",
      "        [0.5037],\n",
      "        [0.4413],\n",
      "        [0.4266],\n",
      "        [0.4029],\n",
      "        [0.4539],\n",
      "        [0.3739],\n",
      "        [0.1313],\n",
      "        [0.2488],\n",
      "        [0.0879],\n",
      "        [0.3229],\n",
      "        [0.4564],\n",
      "        [0.3913],\n",
      "        [0.3040],\n",
      "        [0.4376],\n",
      "        [0.2391],\n",
      "        [0.4117],\n",
      "        [0.4615],\n",
      "        [0.5125],\n",
      "        [0.4284],\n",
      "        [0.3866],\n",
      "        [0.4605],\n",
      "        [0.4424],\n",
      "        [0.3571],\n",
      "        [0.2931],\n",
      "        [0.4940],\n",
      "        [0.3432],\n",
      "        [0.3993],\n",
      "        [0.3261],\n",
      "        [0.3134],\n",
      "        [0.3862],\n",
      "        [0.4141],\n",
      "        [0.4196],\n",
      "        [0.4216],\n",
      "        [0.4880],\n",
      "        [0.4307],\n",
      "        [0.4733],\n",
      "        [0.4401],\n",
      "        [0.3505],\n",
      "        [0.4781],\n",
      "        [0.3209],\n",
      "        [0.4371],\n",
      "        [0.4231],\n",
      "        [0.3836],\n",
      "        [0.3836],\n",
      "        [0.3544],\n",
      "        [0.3323],\n",
      "        [0.3366],\n",
      "        [0.2697],\n",
      "        [0.3678],\n",
      "        [0.3013],\n",
      "        [0.2039],\n",
      "        [0.4018],\n",
      "        [0.4088],\n",
      "        [0.4455],\n",
      "        [0.3369],\n",
      "        [0.4160],\n",
      "        [0.4745],\n",
      "        [0.4043],\n",
      "        [0.3503],\n",
      "        [0.3755],\n",
      "        [0.5324],\n",
      "        [0.3115],\n",
      "        [0.3679],\n",
      "        [0.4694],\n",
      "        [0.3805],\n",
      "        [0.3963],\n",
      "        [0.4013],\n",
      "        [0.3105],\n",
      "        [0.3208],\n",
      "        [0.3012],\n",
      "        [0.4004],\n",
      "        [0.4059],\n",
      "        [0.2615],\n",
      "        [0.3653],\n",
      "        [0.4647],\n",
      "        [0.4255],\n",
      "        [0.3298],\n",
      "        [0.3131],\n",
      "        [0.4158],\n",
      "        [0.4181],\n",
      "        [0.4717],\n",
      "        [0.4862],\n",
      "        [0.4498],\n",
      "        [0.4016],\n",
      "        [0.3442],\n",
      "        [0.3877],\n",
      "        [0.3610],\n",
      "        [0.3941],\n",
      "        [0.3862],\n",
      "        [0.2578],\n",
      "        [0.3789],\n",
      "        [0.4524],\n",
      "        [0.3438],\n",
      "        [0.3555],\n",
      "        [0.3613],\n",
      "        [0.4537],\n",
      "        [0.5076],\n",
      "        [0.3703],\n",
      "        [0.4652],\n",
      "        [0.4302],\n",
      "        [0.3694],\n",
      "        [0.4700],\n",
      "        [0.3631],\n",
      "        [0.2862],\n",
      "        [0.3460],\n",
      "        [0.3595],\n",
      "        [0.3739],\n",
      "        [0.4291],\n",
      "        [0.4136],\n",
      "        [0.4558],\n",
      "        [0.4442]], grad_fn=<AddmmBackward0>)\n",
      "Epoch 5/5:\n",
      "loss tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0420, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0400, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0765, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0334, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0669, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0248, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0479, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0657, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0469, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1016, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0536, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1572, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0680, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0696, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0978, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0309, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0742, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0486, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0413, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.1271, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0554, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0553, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0904, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0585, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0259, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0316, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0419, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0345, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0918, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
      "loss tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
      "sequence_id R5H254.1/52-253\n",
      "embedding tensor([[ 0.2384,  0.3043,  0.0256,  ...,  0.5250, -0.2832,  0.3937],\n",
      "        [ 0.2234, -0.0721,  0.2491,  ..., -0.4389, -0.0534,  0.3469],\n",
      "        [ 0.4049, -0.2315,  0.3235,  ...,  0.1542, -0.2983,  0.3671],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0638,  0.6679,  ...,  0.3376,  0.1466, -0.1909],\n",
      "        [ 0.2008, -0.3574,  0.8158,  ..., -0.4665,  0.0776, -0.1631],\n",
      "        [ 0.1929, -0.6149,  0.4540,  ..., -0.2101,  0.1160,  0.3863]])\n",
      "loss tensor(0.0710, grad_fn=<MseLossBackward0>)\n",
      "label torch.Size([202])\n",
      "label tensor([0.4373, 0.6094, 0.5547, 0.3411, 0.4690, 0.4895, 0.4565, 0.4307, 0.3643,\n",
      "        0.3276, 0.3018, 0.2981, 0.4307, 0.4724, 0.5576, 0.5479, 0.5303, 0.3533,\n",
      "        0.6523, 0.6143, 0.2715, 0.4783, 0.4041, 0.6929, 0.5464, 0.3198, 0.1965,\n",
      "        0.3127, 0.3838, 0.3079, 0.1470, 0.4885, 0.3545, 0.5029, 0.0000, 0.0000,\n",
      "        0.0000, 0.2365, 0.1185, 0.1571, 0.1676, 0.4695, 0.1456, 0.0629, 0.0800,\n",
      "        0.0545, 0.0662, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0673, 0.1559,\n",
      "        0.0628, 0.1274, 0.0865, 0.1105, 0.1045, 0.1709, 0.3691, 0.3716, 0.2378,\n",
      "        0.2783, 0.4988, 0.3423, 0.1442, 0.3870, 0.5122, 0.3167, 0.3582, 0.2008,\n",
      "        0.1316, 0.2260, 0.0844, 0.2129, 0.0942, 0.0777, 0.0806, 0.0834, 0.0818,\n",
      "        0.0864, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0607, 0.0919, 0.0573, 0.2683, 0.1915, 0.3979, 0.1793, 0.6035,\n",
      "        0.6934, 0.3159, 0.1014, 0.2852, 0.0589, 0.1846, 0.1081, 0.1271, 0.0620,\n",
      "        0.1670, 0.1667, 0.1103, 0.4636, 0.3340, 0.2489, 0.1440, 0.3770, 0.2120,\n",
      "        0.1259, 0.1548, 0.1863, 0.4001, 0.4094, 0.1315, 0.0789, 0.6318, 0.3757,\n",
      "        0.5635, 0.1370, 0.7856, 0.0933, 0.5952, 0.0762, 0.3369, 0.1676, 0.1599,\n",
      "        0.1001, 0.1090, 0.0402, 0.0000, 0.0307, 0.0330, 0.0429, 0.0559, 0.0659,\n",
      "        0.0419, 0.1829, 0.0000, 0.1605, 0.1384, 0.2583, 0.0526, 0.3179, 0.2206,\n",
      "        0.0807, 0.0883, 0.0986, 0.1127, 0.1667, 0.1062, 0.1437, 0.0953, 0.0000,\n",
      "        0.0000, 0.0000, 0.2113, 0.0934, 0.1433, 0.1127, 0.1996, 0.1202, 0.0530,\n",
      "        0.0554, 0.1213, 0.1206, 0.7100, 0.2167, 0.1885, 0.1978, 0.1443, 0.1659,\n",
      "        0.0461, 0.3103, 0.7817, 0.3196])\n",
      "embedding torch.Size([202, 320])\n",
      "embedding tensor([[ 0.0315, -0.0691, -0.2562,  ...,  0.6433, -0.1273, -0.1903],\n",
      "        [-0.1440, -0.2611,  0.5538,  ...,  0.1722, -0.1411,  0.2378],\n",
      "        [-0.1277, -0.1067,  0.8156,  ...,  0.5884,  0.2388,  0.4726],\n",
      "        ...,\n",
      "        [ 0.1113, -0.0343,  0.3474,  ..., -0.2219,  0.2560, -0.2116],\n",
      "        [ 0.0499, -0.0368,  0.1926,  ..., -0.1841, -0.0107, -0.1313],\n",
      "        [ 0.0466, -0.1913, -0.2238,  ...,  0.0975,  0.2825,  0.0754]])\n",
      "output torch.Size([202, 1])\n",
      "output tensor([[0.6068],\n",
      "        [0.3651],\n",
      "        [0.3878],\n",
      "        [0.3925],\n",
      "        [0.3139],\n",
      "        [0.2518],\n",
      "        [0.4850],\n",
      "        [0.2238],\n",
      "        [0.2829],\n",
      "        [0.3957],\n",
      "        [0.4025],\n",
      "        [0.3502],\n",
      "        [0.1764],\n",
      "        [0.2375],\n",
      "        [0.4166],\n",
      "        [0.3552],\n",
      "        [0.2595],\n",
      "        [0.5149],\n",
      "        [0.4838],\n",
      "        [0.3391],\n",
      "        [0.2002],\n",
      "        [0.4487],\n",
      "        [0.3996],\n",
      "        [0.3830],\n",
      "        [0.3294],\n",
      "        [0.4018],\n",
      "        [0.4688],\n",
      "        [0.3189],\n",
      "        [0.3290],\n",
      "        [0.4037],\n",
      "        [0.2164],\n",
      "        [0.3912],\n",
      "        [0.3947],\n",
      "        [0.3366],\n",
      "        [0.3651],\n",
      "        [0.2024],\n",
      "        [0.3083],\n",
      "        [0.3432],\n",
      "        [0.3314],\n",
      "        [0.2959],\n",
      "        [0.3674],\n",
      "        [0.3745],\n",
      "        [0.2807],\n",
      "        [0.4986],\n",
      "        [0.3901],\n",
      "        [0.4063],\n",
      "        [0.3617],\n",
      "        [0.4457],\n",
      "        [0.4414],\n",
      "        [0.4781],\n",
      "        [0.4067],\n",
      "        [0.2547],\n",
      "        [0.4260],\n",
      "        [0.3773],\n",
      "        [0.4651],\n",
      "        [0.3569],\n",
      "        [0.3399],\n",
      "        [0.3191],\n",
      "        [0.2180],\n",
      "        [0.3684],\n",
      "        [0.2699],\n",
      "        [0.3625],\n",
      "        [0.2925],\n",
      "        [0.3814],\n",
      "        [0.4815],\n",
      "        [0.3544],\n",
      "        [0.3682],\n",
      "        [0.2923],\n",
      "        [0.3243],\n",
      "        [0.4081],\n",
      "        [0.4017],\n",
      "        [0.3842],\n",
      "        [0.3638],\n",
      "        [0.4286],\n",
      "        [0.3711],\n",
      "        [0.4036],\n",
      "        [0.3543],\n",
      "        [0.4015],\n",
      "        [0.4076],\n",
      "        [0.3836],\n",
      "        [0.3464],\n",
      "        [0.4262],\n",
      "        [0.3078],\n",
      "        [0.3844],\n",
      "        [0.3668],\n",
      "        [0.3788],\n",
      "        [0.4428],\n",
      "        [0.4362],\n",
      "        [0.3972],\n",
      "        [0.5253],\n",
      "        [0.3995],\n",
      "        [0.4995],\n",
      "        [0.4385],\n",
      "        [0.4254],\n",
      "        [0.4035],\n",
      "        [0.4525],\n",
      "        [0.3651],\n",
      "        [0.1245],\n",
      "        [0.2553],\n",
      "        [0.0823],\n",
      "        [0.3194],\n",
      "        [0.4512],\n",
      "        [0.3880],\n",
      "        [0.2998],\n",
      "        [0.4319],\n",
      "        [0.2409],\n",
      "        [0.4145],\n",
      "        [0.4583],\n",
      "        [0.5070],\n",
      "        [0.4277],\n",
      "        [0.3877],\n",
      "        [0.4517],\n",
      "        [0.4425],\n",
      "        [0.3574],\n",
      "        [0.2957],\n",
      "        [0.4876],\n",
      "        [0.3390],\n",
      "        [0.4041],\n",
      "        [0.3249],\n",
      "        [0.3147],\n",
      "        [0.3838],\n",
      "        [0.4088],\n",
      "        [0.4183],\n",
      "        [0.4155],\n",
      "        [0.4881],\n",
      "        [0.4270],\n",
      "        [0.4681],\n",
      "        [0.4349],\n",
      "        [0.3550],\n",
      "        [0.4778],\n",
      "        [0.3254],\n",
      "        [0.4346],\n",
      "        [0.4170],\n",
      "        [0.3752],\n",
      "        [0.3871],\n",
      "        [0.3505],\n",
      "        [0.3280],\n",
      "        [0.3320],\n",
      "        [0.2668],\n",
      "        [0.3612],\n",
      "        [0.3003],\n",
      "        [0.2060],\n",
      "        [0.4032],\n",
      "        [0.4054],\n",
      "        [0.4440],\n",
      "        [0.3418],\n",
      "        [0.4170],\n",
      "        [0.4737],\n",
      "        [0.4047],\n",
      "        [0.3525],\n",
      "        [0.3734],\n",
      "        [0.5246],\n",
      "        [0.3137],\n",
      "        [0.3617],\n",
      "        [0.4604],\n",
      "        [0.3778],\n",
      "        [0.3900],\n",
      "        [0.3993],\n",
      "        [0.3113],\n",
      "        [0.3261],\n",
      "        [0.3058],\n",
      "        [0.3916],\n",
      "        [0.4034],\n",
      "        [0.2630],\n",
      "        [0.3697],\n",
      "        [0.4542],\n",
      "        [0.4161],\n",
      "        [0.3336],\n",
      "        [0.3135],\n",
      "        [0.4147],\n",
      "        [0.4114],\n",
      "        [0.4743],\n",
      "        [0.4878],\n",
      "        [0.4407],\n",
      "        [0.3988],\n",
      "        [0.3414],\n",
      "        [0.3910],\n",
      "        [0.3601],\n",
      "        [0.3883],\n",
      "        [0.3796],\n",
      "        [0.2594],\n",
      "        [0.3793],\n",
      "        [0.4459],\n",
      "        [0.3445],\n",
      "        [0.3583],\n",
      "        [0.3635],\n",
      "        [0.4487],\n",
      "        [0.4997],\n",
      "        [0.3706],\n",
      "        [0.4591],\n",
      "        [0.4234],\n",
      "        [0.3666],\n",
      "        [0.4698],\n",
      "        [0.3583],\n",
      "        [0.2854],\n",
      "        [0.3493],\n",
      "        [0.3606],\n",
      "        [0.3770],\n",
      "        [0.4290],\n",
      "        [0.4164],\n",
      "        [0.4585],\n",
      "        [0.4333]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fonction pour obtenir les embeddings correspondant aux séquences\n",
    "def get_embeddings(sequences, embeddings_dict):\n",
    "    embeddings = []\n",
    "    for sequence_id in sequences:\n",
    "        embedding = embeddings_dict[sequence_id]\n",
    "        print(embedding)\n",
    "        embeddings.append(embedding)\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding(sequence_id, embeddings_dict):\n",
    "    return embeddings_dict[sequence_id]\n",
    "\n",
    "\n",
    "# Fonction pour entraîner le modèle en utilisant la descente de gradient stochastique (SGD)\n",
    "def train_model_stochastic(model, optimizer, loss_fn, sequences, conservation_scores):\n",
    "    model.train()\n",
    "    for i in range(len(sequences)):\n",
    "        sequence_id = sequences[i]\n",
    "        if i == len(sequences) - 1:\n",
    "            print(\"sequence_id\", sequence_id)\n",
    "            print(\"embedding\", embedding)\n",
    "        embedding = get_embedding(sequence_id, embeddings_dict)\n",
    "        #embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        label = torch.tensor(conservation_scores[i], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(embedding)\n",
    "\n",
    "        \n",
    "        loss = loss_fn(output.squeeze(), label)\n",
    "        print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i == len(sequences) - 1 or False:\n",
    "            print(\"label\", label.shape)\n",
    "            print(\"label\", label)\n",
    "            print(\"embedding\", embedding.shape)\n",
    "            print(\"embedding\", embedding)\n",
    "            print(\"output\", output.shape)\n",
    "            print(\"output\", output)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# Définir le modèle de régression linéaire\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de validation\n",
    "def evaluate_model(model, loss_fn, data_loader):\n",
    "    running_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "# Configuration des hyperparamètres\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# Créer l'ensemble de données\n",
    "dataset = [(embeddings_dict[sequence], conservation_scores) for sequence, conservation_scores in zip(sequences, conservation_scores_tensors)]\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Créer les data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialiser le modèle, la fonction de perte et l'optimiseur\n",
    "model = LinearRegression(input_size=320)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_losses = []\n",
    "# Entraînement du modèle\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "    model.train()\n",
    "    train_model_stochastic(model, optimizer, loss_fn,\n",
    "                           sequences[:100], conservation_scores_tensors[:100])\n",
    "\n",
    "    # Validation du modèle\n",
    "    model.eval()\n",
    "    val_loss = evaluate_model(model, loss_fn, val_loader)\n",
    "    val_losses.append(val_loss) \n",
    "    #print(f'Validation Loss: {val_loss}')\n",
    "# on trace la perte de validation au fil des époques\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "345f4348-f648-4199-9e54-f67964b2343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1827,  0.4544,  0.0703,  ...,  0.9286, -0.1682,  0.0573],\n",
      "        [-0.0597, -0.0637,  0.7307,  ...,  0.3019,  0.2616,  0.5997],\n",
      "        [ 0.1731, -0.0389,  0.8230,  ...,  0.5839, -0.2457,  0.3610],\n",
      "        ...,\n",
      "        [-0.0797,  0.2541,  0.7844,  ...,  0.5837,  0.1552,  0.1666],\n",
      "        [ 0.2382, -0.3610,  0.6018,  ..., -0.0876, -0.3903,  0.0103],\n",
      "        [-0.0087, -0.5144,  0.2969,  ..., -0.0461, -0.0331,  0.4260]])\n"
     ]
    }
   ],
   "source": [
    "# Cargar el archivo .pt\n",
    "embedding = torch.load('curated_dataset/example_embeddings_esm2_reduced_input/A0A1I4YI28.1/1-164.pt')\n",
    "\n",
    "# Ahora puedes acceder al tensor del embedding\n",
    "print(embedding[\"representations\"][6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f806362-69bc-477d-8d1a-841b088632e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi_entorno",
   "language": "python",
   "name": "mi_entorno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
