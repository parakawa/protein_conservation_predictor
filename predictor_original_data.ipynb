{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6617040a-0a00-4aab-9cd9-d8da5afc1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503531e4-e847-4563-8092-2d81d170d992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_dict.shape 0\n",
      "Se ha guardado el diccionario de embeddings en un archivo Excel.\n"
     ]
    }
   ],
   "source": [
    "def get_embeddings_vectors_curated_data(folder_path):\n",
    "    # Initialize a list to store the vectors\n",
    "    embeddings = {}\n",
    "\n",
    "    # Traverse through each folder in the specified directory\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file is a .pt file\n",
    "            if file.endswith('.pt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                embedding = torch.load(file_path)\n",
    "                embeddings[embedding[\"label\"]] = embedding[\"representations\"][6]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Obtener los embeddings vectors\n",
    "embeddings_dict = get_embeddings_vectors_curated_data('curated_dataset/example_embeddings_esm2_version_2')\n",
    "print(\"embeddings_dict.shape\", len(embeddings_dict))\n",
    "\n",
    "# Convertir el diccionario a un DataFrame de pandas\n",
    "#df = pd.DataFrame(list(embeddings_dict.items()), columns=['Key', 'Value'])\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel\n",
    "#df.to_excel('embeddings_dict.xlsx', index=False)\n",
    "\n",
    "print(\"Se ha guardado el diccionario de embeddings en un archivo Excel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a601237d-39a5-4ee2-9e82-674d099de4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conservation_scores(csv_file):\n",
    "    # Charger le CSV\n",
    "    df = pd.read_csv(csv_file, delimiter=',', names=[\n",
    "                     'sequence id', 'conservation score'], header=0)\n",
    "\n",
    "    sequences = df['sequence id'].values\n",
    "    conservation_scores = df['conservation score'].apply(lambda x: np.array(\n",
    "    [float(i) if i != 'nan' else 0.0 for i in x.split()], dtype=np.float32)).values\n",
    "    #print(\"conservation_scores\", conservation_scores)\n",
    "\n",
    "    return sequences, conservation_scores\n",
    "\n",
    "sequences, conservation_scores_tensors = get_conservation_scores('curated_dataset/conservation_scores_formated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f37ff17-4e49-4642-94cd-10eef15542b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences len 35871\n",
      "conservation_scores_tensors 35871\n"
     ]
    }
   ],
   "source": [
    "print(\"sequences len\", len(sequences))\n",
    "print(\"conservation_scores_tensors\", len(conservation_scores_tensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9e05a85-f664-4bbb-ba7e-ff66dc69011d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A0A009J727.1/56-242'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 105\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 105\u001b[0m \u001b[43mtrain_model_stochastic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconservation_scores_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Validation du modèle\u001b[39;00m\n\u001b[1;32m    109\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[26], line 25\u001b[0m, in \u001b[0;36mtrain_model_stochastic\u001b[0;34m(model, optimizer, loss_fn, sequences, conservation_scores)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequence_id)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding)\n\u001b[0;32m---> 25\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(conservation_scores[i], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[26], line 14\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(sequence_id, embeddings_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(sequence_id, embeddings_dict):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43membeddings_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'A0A009J727.1/56-242'"
     ]
    }
   ],
   "source": [
    "print(\"embeddings\", embeddings_dict)\n",
    "\n",
    "# Fonction pour obtenir les embeddings correspondant aux séquences\n",
    "def get_embeddings(sequences, embeddings_dict):\n",
    "    embeddings = []\n",
    "    for sequence_id in sequences:\n",
    "        embedding = embeddings_dict[sequence_id]\n",
    "        print(embedding)\n",
    "        embeddings.append(embedding)\n",
    "    embeddings = torch.stack(embeddings)\n",
    "    return embeddings\n",
    "\n",
    "def get_embedding(sequence_id, embeddings_dict):\n",
    "    return embeddings_dict[sequence_id]\n",
    "\n",
    "\n",
    "# Fonction pour entraîner le modèle en utilisant la descente de gradient stochastique (SGD)\n",
    "def train_model_stochastic(model, optimizer, loss_fn, sequences, conservation_scores):\n",
    "    model.train()\n",
    "    for i in range(len(sequences)):\n",
    "        sequence_id = sequences[i]\n",
    "        if i == len(sequences) - 1:\n",
    "            print(\"sequence_id\", sequence_id)\n",
    "            print(\"embedding\", embedding)\n",
    "        embedding = get_embedding(sequence_id, embeddings_dict)\n",
    "        #embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "        label = torch.tensor(conservation_scores[i], dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(embedding).squeeze()\n",
    "\n",
    "        \n",
    "        loss = loss_fn(output, label)\n",
    "        print(\"loss\", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i == len(sequences) - 1 or False:\n",
    "            print(\"label shape\", label.shape)\n",
    "            print(\"label\", label)\n",
    "            print(\"embedding shape\", embedding.shape)\n",
    "            print(\"embedding\", embedding)\n",
    "            print(\"output shape\", output.shape)\n",
    "            print(\"output\", output)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "# Définir les modèles de régression linéaire, Multi layer perceptron\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Évaluation du modèle sur l'ensemble de validation\n",
    "def evaluate_model(model, loss_fn, data_loader):\n",
    "    running_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "# Configuration des hyperparamètres\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# Créer l'ensemble de données\n",
    "dataset = [(embeddings_dict.get(sequence), conservation_scores) for sequence, conservation_scores in zip(sequences, conservation_scores_tensors)]\n",
    "#dataset = [(embeddings_dict[sequence], conservation_scores) for sequence, conservation_scores in zip(sequences, conservation_scores_tensors)]\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Créer les data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialiser le modèle, la fonction de perte et l'optimiseur\n",
    "model = LinearRegression(input_size=320)\n",
    "#model = MLP(input_size=320, hidden_size=64, output_size=1)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "val_losses = []\n",
    "# Entraînement du modèle\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "    model.train()\n",
    "    train_model_stochastic(model, optimizer, loss_fn,\n",
    "                           sequences, conservation_scores_tensors)\n",
    "\n",
    "    # Validation du modèle\n",
    "    model.eval()\n",
    "    val_loss = evaluate_model(model, loss_fn, val_loader)\n",
    "    val_losses.append(val_loss) \n",
    "    #print(f'Validation Loss: {val_loss}')\n",
    "# on trace la perte de validation au fil des époques\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss over Epochs')\n",
    "plt.show()\n",
    "print(\"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ab5c7e-74e6-4462-9815-5a3d05a1114a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
